---
title: "Measuring Volatility Spillovers and Connectedness"
author: Rhamey Sayed
output: 
  bookdown::html_document2:
    code_folding: hide
    number_sections: false
    fig_caption: true
---

### NOTE:

The dashboard which this vignette supports can be found here: [Dashboard](https://rhamey-sayed.shinyapps.io/SpilloversConnectedness/)

Also the full code is on [Github](https://github.com/rhameysayed/Spillovers-and-Connectedness)

## Section 1: Introduction

### Section 1.1: An Analogy

Imagine a container full of particles bouncing around. We can measure the energy of the entire container and the amount of energy of each individual particle. But what if we want to measure the effect of one particle's energy on other particles? The entire system? If we assume there is one particle with the highest amount of energy we can conclude the particles nearest to it derive a some of their energy from that highest energy particle and those particles pass it to the ones near them and so on. If we want to predict the energy of one of these particles we would be misled if we did not consider the particles nearby and only analyzed each particle individually as an independent observation. 

If we replace particles with stock prices, and energy with volatility of stock returns, then our container becomes an index of stocks. We expect stocks within the same industry to be correlated with each other (i.e. to share energy), and some stocks to "share" their volatility with some more than others. Measuring and predicting the volatility in returns of each stock independently will similarly miss other drivers as in the particle example. 

Standard statistical and econometric methods such (vector) autoregressions assume (or impose) independence of variables, but it will be shown below that relaxing the assumption of independence does not improve a method's forecast but it does enable researchers to better isolate other drivers of volatility in stock price return volatility. It is clear that in the world of finance, especially the stock market, the choice of variables matters because financial variables are often highly correlated and it is feasible to imagine the price oil being correlated with Google's stock price for instance. Hence it is very important that the selection of variables be correlated with reason. 

### Section 1.2 Overview

This vignette combines and replicates the work done in [Dieblod and Yilmaz 2009](http://www.ssc.upenn.edu/~fdiebold/papers/paper75/DY2final.pdf), [DY 2010](http://www.ssc.upenn.edu/~fdiebold/papers/paper99/DirectionofSpillovers_Mar10.pdf), and [DY 2011](http://www.ssc.upenn.edu/~fdiebold/papers/paper106/dieboldandyilmaz2011.pdf), as well as scaling the methods for quick application and replication. The DY papers describe how to quantify in an index the amount of forecast error that can be attributed to shocks of the error terms of each dependent variable in vector autoregression. Consider a set of stocks for which we want to estimate and forecast either returns or volatility, and control for the systematic, or common, factor driving each one, then whatever cannot be explained by common factors is idiosyncratic error. Then an interesting question to ask is whether these idiosyncratic errors of some stocks can effect the forecast others. In other words, do idiosyncratic errors spillover into common factors' ability to forecast? These "spillovers" are found using standard time-series techniques for finding impulse-response functions and forecast error variance decompositions. In addition, DY 2011 show how the effects of these shocks can be represented as a network when one relaxes the assumptions that vector autoregressions have normally distributed, i.i.d. error terms and that shocks to each variable must be orthogonal to the other shocks. In the spirit of DY 2011, stocks of large banks and AIG will be the subjects of analysis. Like all the DY publications, the analysis will focus on connections and spillovers of volatility.

This vignette will proceed as follows: Section 2 will describe the data source and summarize the data; Section 3 will motivate the concepts of spillovers and connectedness using a analysis of the entire sample of data; Section 4 will use a rolling window over the time-series data, applying the same concepts from the full sample analysis to derive an index of spillover/conncectedness and measure behavior of the network over time; Section 5 will apply the methods developed to study the effect of the dollar exchange rate on the Collateralized Loan Obligation (CLO) market building on research from [Niepmann & Schmidt-Eisenlohr](https://voxeu.org/article/when-dollar-appreciates-us-corporate-credit-tightens).

## Section 2: Data

The data is pulled from Yahoo! Finance using the getSymbols() function from the quantmod package, which includes Open, Close, High, and Low prices for a user specified set of stocks/indexes over a chosen period of time measured in days. The data is similar to that in [DY 2010](http://www.ssc.upenn.edu/~fdiebold/papers/paper99/DirectionofSpillovers_Mar10.pdf), so measures of stock return volatility used in the analysis and app are similar to that publication. First daily variance for stock $i$ at time $t$ is estimated using high and low prices: 

\begin{equation}
  \label{eq:variance}
  \sigma^2_{it} = 0.361[ln(P^{max}_{i,t}) - ln(P^{min}_{i,t-1}))]^2 
\end{equation} 

To analyze the volatility of closed-end funds the minimum price is observed at $t-1$, since High, Low, Open, and Close prices on any given day are the same for these funds. Hence, returns and volatilities for these funds would be zero when using prices recorded on the same day.

Since volatilities tend to be skewed, it is common practice to use log-volatilities which closely approximate a normal distribution. However, to control for instances when volatility is zero, $sinh^{-1}$ is used instead of the natural logarithm ($sinh^{-1}(x) = log(2x)$). 

\begin{equation} 
  \sigma_{it} = sinh^{-1}(\sqrt{252*\sigma^2_{it}})
  \label{eq:stdev}
\end{equation}

Using the quantmod getSymbols function, read in the set of stock prices for Barclays (BK), Bank of Nova Scotia (BNS), TD Bank (TD), Well Fargo (WFC), Goldman Sachs (GS), JP Morgan Chase & Co. (JPM), Morgan Stanley (MS), Bank of America (BAC), Credit Suisse (CS), Deutsche Bank (DB), HSBC, Citibank (C), and AIG over time period spanning January 2, 2006 to December 30 2016. This time period capture the run up, aftermath, and recovery of the 2008 financial crisis. The daily volatilities which are shown in Figure (\@ref(fig:annvol)) and summarized in Table 1.

```{r echo=FALSE,message=FALSE,warning=FALSE}
source("C:\\Users\\Rhamey\\Desktop\\financeR\\spillover_connectedness_functions.R")
library(kableExtra)

tickers <- c("BK","BNS","TD","WFC","GS","JPM","MS","BAC","CS","DB","HSBC","C","AIG")
tickers1 = tickers[order(tickers)]

start_date = "2006-01-02"
end_date = "2016-12-30"
### read symbols from yahoo using getSymbols from quantmod package
data1 <- new.env()
getSymbols(tickers1,src="yahoo",from=as.POSIXct(start_date),to=as.POSIXct(end_date),env=data1)

### Calculate daily volatility using Open and Close prices
### This formula is found in section 3 of DY 2010
vol.fn <- function(history){asinh(sqrt(252*0.361*log(history[,3]/lag(history[,2]))^2))}
vol1 <- lapply(data1,vol.fn)

## combine each ticker's xts object into one xts object
vol.data <- Reduce(merge.all,vol1)
vol.data <- na.omit(vol.data)
## remove the suffixes from the names
vol1.names = colnames(vol.data)
newNames = sapply(as.character(vol1.names),function(x) substr(x,1,nchar(x)-4))
colnames(vol.data) <- newNames
vol_df_melt <- melt(as.data.frame(vol.data))
colnames(vol_df_melt) <- c("Name","Volatility")

data_summary <- vol_df_melt %>% group_by(Name) %>% summarise(Count = n(), Mean = mean(Volatility), Median = median(Volatility), SD = sd(Volatility), Min = min(Volatility), Max = max(Volatility))

kable(data_summary,caption = "Summary Table: Annualized Volatility") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```

```{r annvol, fig.width=14,fig.height=8,fig.cap="Annualized Log Volatility"}
chart.TimeSeries(vol.data,lwd=2,auto.grid=F,ylab="Annualized Log Volatility",xlab="Time",
                     main="Log Volatility",lty=1,
                     legend.loc="topright")
```


## Section 3: Full Sample Analysis
## Section 3.1: Basic Time-Series: VAR(p), MA(q), Impulse-Response, Forecast Error Variance

In time-series analysis (and most regressions generally) the most interesting information is found in the distribution of the error terms, especially the variance. In a multivariate model the distribution of error terms embeds the assumptions made regarding correlations of dependent variables (i.e. a diagonal covariance matrix assumes independence). In financial econometrics forecasting stock price returns or volatility entails two components - systematic and idiosyncratic - codified as a regression:

\begin{align}
  \sigma_{it} = \beta \sigma_{M,t-1} + \epsilon_{it}
  \label{eq:beta}
\end{align}

Where $\sigma_{it}$ is the volatility of stock $i$ at time $t$, $\sigma_M$ is the market volatility (i.e. the volatility on the S&P 500, Dow Jones, etc.), $\beta$ is a measure of the systematic portion of a stock's volatility, or the strength/measure of its relationship with the broader market, and $\epsilon$ is the idiosyncratic portion that pertains to stock $i$. The interesting question is how responsive is the future volatility $\sigma_{i,t+1}$, at time $t+1$, to a shock to $\epsilon_{it}$ ? And in a system of several different volatilities, how does a shock to the idiosyncratic term, $\epsilon_{jt}$ , of variable $j$ at time $t$ effect $\sigma_{i,t+1}$ for variable $i$ at time $t+1$ ? In other words, we are interested in finding whether and how idiosyncratic risks of one stock's volatility effect the future volatilities of other stocks. These effects are known as spillovers or connectedness.

To derive a measure of these spillovers, the calculations proceed in four steps: 

1) Estimate a vector autoregression (VAR) with $P$ lags for a set of $N$ variables of $\sigma_i$ for $i = 1,..,N$ variables which will replace $\sigma_M$:

\begin{equation}
  \sigma_{t} = \sum^P_{l=1} \beta_{t-l}\sigma_{t-l} + \epsilon_{t}
\end{equation}

where $\sigma_{t}$ and $\sigma_{t-l}$ are vectors of length $N$, each $\beta_{t-l}$ is an $NxN$ coefficient matrix, and $\epsilon_t$ is a vector of error terms with a distribution $N(0,\sigma^2)$.

2) convert the VAR into a moving average representation of order $Q$ ($MA(Q)$):

\begin{equation}
\sigma_t = \sum^Q_{i=0} \phi_{t-i}\epsilon_{t-i}
\end{equation}

Where each $\phi$ is a matrix of coefficients which measure the magnitudes of each impulse. Here an impulse is defined as a unitary shock to $\epsilon_{t-i}$.

3) derive the impulse-response function (IRF) for each variable using the $MA(Q)$ for each variable $j$ on variable $i$.

\begin{equation}
\sum^Q_{l=0} (e'_i A_l P e_j)^2
\end{equation}

where $e_i$ and $e_j$ are basis vectors with unity at $i$ and $j$ respectively, $A$ is a matrix of $\phi$ cefficients. The impulse response is the cumulative error of forecast from a shock to variable $i$ from variable $j$ at time $t-l$.

4) calculate forecast error variance decomposition matrix (FEVD) for the entire set of variables using the IRF. 

\begin{equation}
  \theta^o_{ij} = \frac{\sum^n_{l=0} (e'_i A_l P e_j)^2}{\sum^n_{l=0} (e'_i A_l \Sigma A'_l e_i)}
  \label{eq:fevd}
\end{equation}

The numerator is just the impulse response and the denominator is the total forecast error in a system of variables and $\Sigma$ is the error covariance marix. The forecast error variance of varaible $i$ attributable to variable $j$ is the impulse respnse of $j$ on $i$ divided by the total forecast error of $i$.


For textbook treatment of how this is done algebraically see  [Zivot](https://faculty.washington.edu/ezivot/econ584/notes/varModels.pdf), [Cochrane Chapter 7](http://econ.lse.ac.uk/staff/wdenhaan/teach/cochrane.pdf), and [Pesaran and Shin 1997](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.5596&rep=rep1&type=pdf) Section 2 for estimating both correlated an uncorrelated shocks.

## Section 3.2: Full Sample Spillover Table
### Step 1. Vector Autoregression (VAR)

The difference between spillovers and connectedness in the DY papers is the difference between assuming errors are orthogonal versus correlated. To estimate a model with correlated errors is as simple as feeding a VAR the set of variables one wants to analyze. To estimate a model with orthogonal errors using variables which are correlated requires imposing a structure on the VAR to achieve independent error terms. The vars package in R has VAR and SVAR functions [vars](https://cran.r-project.org/web/packages/vars/vars.pdf) The SVAR function estimates a Structural VAR by giving it a matrix that is structured according to a set of assumptions. For this excercise I used a lower triangular matrix of coefficients with 1s along the diagonal as the [Sims critique stipulates](https://www.pauldeng.com/pdf/Sims%20macroeconomics%20and%20reality.pdf). For a quick overview of Sims see [Section 11.4.2 of Zivot](https://faculty.washington.edu/ezivot/econ584/notes/varModels.pdf).

```{r}
vol_var = VAR(vol.data,p=3,type="none")
amat <- diag(ncol(vol.data))
amat[lower.tri(amat)] <- NA
vol_svar = SVAR(vol_var,Amat = amat,estmethod = "direct")
### extract residuals of the VAR
res_t <- residuals(vol_var)
svar_ecov <- vol_svar$Sigma.U
```

### Step 2. Moving Average Representation
Generally speaking an type of autoregression of order $p$ can be converted to a moving average of order $q$ as long as the coefficients of the $AR(p)$ are between -1 and 1. The vars package includes a function Phi which will do this conversion for us by specifying the ordoer $q$ of MA we want - in this case 10. From this estimate the MA coefficients are retrieved, along with the residual errors from the VAR, for the next steps.
```{r}
MA_lag <- 10
theta_temp <- Phi(vol_var,nstep = MA_lag)
svar_theta_temp <- Phi(vol_svar,nstep = MA_lag)
### extract MA coefficients
theta.list <- alply(theta_temp,3)
svar_theta_list <- alply(svar_theta_temp,3)
```

### Step 3. Impulse Response Function (IRF)

For the SVAR, the function fevd.matrix in the code snippet implements the IRF and FEVD calculations, taking the covariance matrix of SVAR residuals and MA coefficients as inputs. To find the IRF derive the lower-triangular Cholesky-decomposition of the residuals' covariance matrix. The MA coefficient matrix is impact of the shocks and the lower triangular matrix imposes the independence of the shocks that is assumed in the SVAR.

### Step 4. Forecast Error Variance Decomposition (FEVD)

Multiplying each lag of MA coefficients by the Cholesky matrix, the FEVD is estimated. Picking out the row of MA coefficients for variable $j$ and multiplying those by the shocks from variable $i$ as shown in the formula below. The results are shown in Table 2 - the Spillover Table. (Note: the rows sum to 1 as expected, so we know the calculations are correct)

\begin{equation}
  \theta^o_{ij} = \frac{\sum^n_{l=0} (e'_i A_l P e_j)^2}{\sum^n_{l=0} (e'_i A_l \Sigma A'_l e_i)}
  \label{eq:fevd}
\end{equation}

$\theta^o_{ij}$ is the orthogonal FEVD, $A_l$ is the MA coefficient matrix at lag $l$ out of $n$ lags, $\Sigma$ is the residual covariance matrix, $P$ is the lower triangular matrix, and $e_i$ and $e_j$ are basis vectors with unity at index $i$ and $j$, respectively. The numerator is the sum of all the forecast error over the time horizon $n$ for variable $i$ that can be attributed to variable $j$, and the denominator is the total forecast error variance for variable $i$.

```{r echo=FALSE, message=FALSE}
# ################## SPILLOVER #########################
fevd.matrix <- function(resid.var,ma.coef){
  ### ei and ej are identity matrices, or
  ### each column in ei and ej is a vector with unity at the i/jth element
  ### see equation 2.7 in Pesaran and Shin (1997)
  Qinv <- t(chol(resid.var))
  
  nobs <- dim(Qinv)[1]
  ei <- diag(nobs)
  ej <- diag(nobs)
  
  ### Initialize matrix of network connections
  Dg <- matrix(NA,ncol=nobs,nrow=nobs)
  
  ### Derive Forecast Error Variance Decomposition (FEVD) with 
  ### Generalized Variance Decomposition method
  for(j in 1:(ncol(ej))){
    ### the following steps calculate the denominator
    ### to estimate FEVD on page 3 of Pesaran and Shin (1997)
    ejj <- ej[,j]
    
    denom.step1 <- llply(ma.coef,function(x) t(ejj)%*%x%*%resid.var%*%t(x)%*%ejj)
    denom = Reduce('+',denom.step1)
    for(i in 1:(ncol(ej))){
      ### the following steps calculates equation 2.10 in Pesaran and Shin (1997)
      eii <- ej[,i]
      num.step1 <- llply(ma.coef,function(x) t(ejj)%*%x%*%Qinv%*%eii)
      num.squared <- llply(num.step1,function(x) x^2)
      num <- Reduce('+',num.squared)
      
      Dg[j,i] <- (num/denom)
      
    }
  }
  Dg
}

error.variance.total <- fevd.matrix(svar_ecov,theta.list)

spillover_table <- make.table(error.variance.total,colnames(vol.data),start_date)
spill_table <- 100*round(spillover_table[["table"]],2)

kable(spill_table,caption = "Volatility Spillovers") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

The row sums (excluding the diagonal terms) of spillovers measure how much volatility the stock receives from the system. The column sums measure how much volatility each stock contributes to the system. Hence, each cell in the table is a measure of how much volatility from column $i$ is given to row $j$. One can estimate the net pairwise spillover by subtracting two spillovers. For instance, the spillover from Goldman Sachs (GS) to Morgan Stanley (MS) is `r spill_table["MS","GS"]`, and from MS to GS is `r spill_table["GS","MS"]`. Therefore, the net pairwise spillover from GS to MS is `r spill_table["MS","GS"] - spill_table["GS","MS"]`.

## Section 3.3: Full Sample Connectedness Table

Connectedness requires similar calculations, except that the shocks from IRF are no longer assumed orthogonal. To achieve this, the lower triangular matrix $P$ for $\theta^o_{ij}$ in the numerator is replaced by the covariance matrix of the residuals. The generalized FEVD equation is then:

\begin{equation}
  \theta^g_{ij} = \frac{\sigma^{-1}_{ii} \sum^n_{l=0} (e'_i A_l \Sigma e_j)^2}{\sum^n_{l=0} (e'_i A_l \Sigma A'_l e_i)}
  \label{eq:gfevd}
\end{equation}

The interpretation of the numerator and denominator is the same as in the orthogonal case. The Connectedness Table is shown in Table 3.

```{r echo=FALSE, message=FALSE}
directional.matrix <- function(resid.var,ma.coef){
  ### ei and ej are identity matrices, or
  ### each column in ei and ej is a vector with unity at the i/jth element
  ### see equation 2.7 in Pesaran and Shin (1997)
  cov.error <- cov(resid.var)
  nobs <- dim(cov.error)[1]
  ei <- diag(nobs)
  ej <- diag(nobs)
  
  ### Initialize matrix of network connections
  Dg <- matrix(NA,ncol=nobs,nrow=nobs)
  
  ### Derive Forecast Error Variance Decomposition (FEVD) with 
  ### Generalized Variance Decomposition method
  for(j in 1:(ncol(ej))){
    ### the following steps calculate the denominator
    ### to estimate FEVD on page 3 of Pesaran and Shin (1997)
    ejj <- ej[,j]
    little.sigma <- 1/sqrt(cov.error[j,j])
    
    denom.step1 <- llply(ma.coef,function(x) t(ejj)%*%x%*%cov.error%*%t(x)%*%ejj)
    denom = Reduce('+',denom.step1)
    for(i in 1:(ncol(ej))){
      ### the following steps calculates equation 2.10 in Pesaran and Shin (1997)
      eii <- ei[,i]
      num.step1 <- llply(ma.coef,function(x) t(ejj)%*%x%*%cov.error%*%eii)
      num.squared <- llply(num.step1,function(x) x^2)
      num.sum <- Reduce('+',num.squared)
      num <- little.sigma*num.sum
      
      Dg[j,i] <- (num/denom)
      
    }
  }
  Dg
}

################## Networks #######################

dir.mat <- directional.matrix(res_t,theta.list)
rownames(dir.mat) <- colnames(vol.data)
colnames(dir.mat) <- rownames(dir.mat)

D <- normalize(dir.mat)*100
df.list <- make.table(D,rownames(D),start_date)
df.list <- round(df.list[["table"]],2)

kable(df.list,caption = "Volatility Connectedness") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

The interpretation of connectedness is slightly different from spillovers. The calculation of net pairwise connectedness is the same as spillovers. In addition, DY 2011 demonstrates why the connectedness table can be interpreted as a network in which each entry represents a weight of connections between the nodes. The degree of each node is measured as the column/row sums off-diagonal terms. The row sums are the from-degrees, column sums are to-degrees of each node. The total connectedness (bottom-right entry in the table) is the average degree of the network (to or from since the row sums equal column sums). Since generalized FEVD matrices do not have row/columns sums equal to one, the entries in the connectedness table have been normalized to that end. 

## Section 3.4: Full Sample FEVD as a Network

DY 2011 demonstrates that net pairwise FEVD (transpose of the connectedness table less the connectedness table) defines a weighted, directed network. Thus the connectedness table can be transformed into a net pairwise connectedness network as shown Figure (\@ref(fig:fullNet)) (To see how the network evolves over time, use the app [here](https://rhamey-sayed.shinyapps.io/SpilloversConnectedness/) and click on the Network Visual tab.). Each edge shown in the graph represents a node giving volatility to another node. The connections with strength in the top tenth percentile are shown in the graph below. The four largest contributors in the network are Citibank, AIG, Morgan Stanley, and Bank of America. Citibank had been the largest bank by assets and was largely exposed to derivatives marekt, as was AIG to the extent that it went bankrupt and had to taken over by the US Treasury, Bank of America bought brankrupt Merrill Lynch, and Morgan Stanley had a similar business model to Merrill Lynch and was likely expected to go bankrupt as well during the crisis.

```{r fullNet, echo=FALSE, message=FALSE, fig.width=9,fig.height=9,fig.cap="Full Sample, Net Pairwise Connectedness"}
net.mat = t(D) - D
net.melt1 <- melt(net.mat)
net.melt <- net.melt1 %>% filter(value != 0 )
colnames(net.melt)[3] <- "weight"

### calculate each percentile of the net pairwise connectedness values
### and choose only the top 10%
net.quant <- quantile(net.melt$weight,prob=seq(0,1,by=0.01))
new.net1 <- net.melt[net.melt$weight >= net.quant[90],]
new.net <- new.net1[new.net1[,1] != new.net1[,2],]

### create igraph graph
net.network <- graph.data.frame(new.net,direct=T)
#net.network <- set_edge_attr(net.network, "weight", value= new.net$weight)
E(net.network)$weight <- as.numeric(new.net$weight)
### set graph nodes
#V(net.network)
### set edge colors
E(net.network)$color <- ifelse(E(net.network)$weight >= net.quant[99],"black",
                             ifelse(E(net.network)$weight < net.quant[99] & E(net.network)$weight >= net.quant[95],"red",
                                    ifelse(E(net.network)$weight < net.quant[95] & E(net.network)$weight >= net.quant[90],"orange","blue")))

### set node size
V(net.network)$size <- degree(net.network)/.5

plot(net.network,layout=layout.circle(net.network),
     main="Firm Connectedness \n (Daily Return Volatility)",
     xlab = "black: 1st percentile \n red: 5th percentile \n orange: 10th percentile \n node size: number of edeges connected to the node")

```


## Section 4: Rolling Window - Spillover Vs. Connected

To apply the methods shown above to a rolling window, it is necessary to only define the size of the rolling window, the size of the increment to move the window, the order $q$ of $MA(q)$, and the number lags in the VAR/SVAR, then loop over the data set. The function vector_autoreg in the code snippet takes in the an .xts data frame, window size, increment, AR lag order, and MA lag order. The example below uses a window of 100 days, incrementing 10 days, with $p = 3$ and $q = 10$. It also calls on function "make.table" (to make the spillover and connectedness table) and "normalize" (to normalize the connectedness table) which can be found in the function script on the GitHub page.

```{r echo=FALSE, message=FALSE}
vector_autoreg <- function(theData.xts,window=100,iter=10,ar_lag=3,ma_lag=10){
  ### create a sequence to use as cut offs for the rolling window
  ### estimation. iter determines how far to move the window forward
  dates <- datesFn(theData.xts,window=100,iter=10)
  date_names <- dates[["dates"]]
  date.df <- dates[["date.df"]]
  ndates <- nrow(date.df)
  alldates <- index(theData.xts)

  model_list <- vector("list",length=length(date_names))
  names(model_list) <- date_names
  centrality <- model_list
  ### initialize a list to hold index values
  fevd.list <- model_list
  ### names of the list come from the index of .xts data object
  spillover.table.list <- model_list
  vol.conn.index <- model_list
  ### names of the list come from the index of .xts data object
  network.list <- model_list
  net.df.list <- model_list
  nvars <- ncol(theData.xts)
  
  
  for(d in 1:ndates){
    ### estimate VAR(p) and SVAR(p)
    var_dates <- alldates[alldates >= date.df[d,"start"] & alldates <= date.df[d,"end"]]
    
    var.vol.temp <- VAR(theData.xts[var_dates,],p=ar_lag,type="none")
    amat <- diag(nvars)
    amat[lower.tri(amat)] <- NA
    svar.vol.temp <- SVAR(var.vol.temp,estmethod = "direct",Amat=amat)
    
    model_list[[date_names[d]]] <- list(orthogonal=svar.vol.temp,
                                        non_orthogonal = var.vol.temp)
    ### get MA coefficients
    theta.temp <- Phi(var.vol.temp,nstep=ma_lag)
    svar.theta <- Phi(svar.vol.temp,nstep=ma_lag)
    ### convert array to listn.ahead
    theta.list <- alply(theta.temp,3)
    svar.theta.list <- alply(svar.theta,3)
    ################## SPILLOVER #########################
    svar_ecov <-svar.vol.temp$Sigma.U
    error.variance.total <- fevd.matrix(svar_ecov,svar.theta.list)*100
    spillover.table.list[[date_names[d]]] <- make.table(error.variance.total,colnames(theData.xts),date_names[d])
    
    
    Qinv <- t(chol(svar_ecov))
    irf <- llply(theta.list,function(x) x%*%Qinv)
    num = llply(irf,cond.sum)
    ### calculate own variance shares (i.e. do not subtract the diagonal of the matrix)
    den = llply(irf,get.trace)
    sum.num = sum(unlist(num))
    sum.den = sum(unlist(den))
    S = sum.num*100/sum.den
    fevd.list[[date_names[d]]] <- S
    ########################################################
    
    ################## CONNECTEDNESS #######################
    e.cov <- cov(residuals(var.vol.temp))
    ### create directional network table
    Sconn = directional.matrix(residuals(var.vol.temp),theta.list)
    ### to normalize or not to normalize
    D = normalize(Sconn)*100
    ### calculate total connectedness index level
    vol.conn.index[[date_names[d]]] = (sum(apply(D,1,sum)  - diag(D)))/ncol(D)
    ########################################################
    
    ################## Networks #######################
    rownames(D) <- colnames(theData.xts)
    colnames(D) <- rownames(D)
    
    df.list <- make.table(D,rownames(D),date_names[d])
    
    net.df.list[[date_names[d]]] <- df.list
    ### Create network Graph
    
    ### calculate net pairwise directional connectedness, Diebold pg 4.
    ### the calculation t(D) - D is the net pairwise equivalent of 
    ### the gross net = to - from, the calculation just above.
    
    net.mat = t(D) - D
    net.melt1 <- melt(net.mat)
    net.melt <- net.melt1 %>% filter(value != 0 )
    colnames(net.melt)[3] <- "weight"
    
    ### calculate each percentile of the net pairwise connectedness values
    ### and choose only the top 10%
    net.quant <- quantile(net.melt$weight,prob=seq(0,1,by=0.01))
    new.net1 <- net.melt[net.melt$weight > net.quant[75],]
    new.net <- new.net1[new.net1[,1] != new.net1[,2],]
    
    ### create igraph graph
    
    net.network <- graph.data.frame(new.net,direct=T)
    E(net.network)$weight <- as.numeric(new.net$weight)
    ### set graph nodes
    #V(net.network)
    ### set edge colors
    E(net.network)$color <- ifelse(E(net.network)$weight >= net.quant[99],"black",
                                   ifelse(E(net.network)$weight < net.quant[99] & E(net.network)$weight >= net.quant[95],"red",
                                          ifelse(E(net.network)$weight < net.quant[95] & E(net.network)$weight >= net.quant[90],"orange","blue")))
    
    ### set node size
    V(net.network)$size <- degree(net.network)/.5
    
    network.list[[date_names[d]]] <- net.network
    #####################################################################################
    remove(var.vol.temp,theta.temp,theta.list,e.cov,Sconn,
           num,den,sum.num,sum.den,S)
  }
  
  list(models = model_list,
       spillover_table = spillover.table.list,
       spill = fevd.list,
       conn=vol.conn.index,
       nets = network.list,
       net_df = net.df.list,
       dates = date_names)
  
}

rolling_var <- vector_autoreg(theData.xts = vol.data)
```

# Section 4.1: Spillover & Connectedness Indexes

```{r indexes, fig.width=14,fig.height=8,fig.cap="Spillover(Orthogonal) and Connectedness(Non-Orthogonal) Indexes"}
############## CONNECTEDNESS ################
vol.conn.index.out <- rolling_var[["conn"]]
### unlist output data
vol.conn.index.df <- data.frame(unlist(vol.conn.index.out))
colnames(vol.conn.index.df) = c("Index")
### .xts object
vol.conn.index.df$Date <- as.POSIXct(rownames(vol.conn.index.df),format="%Y-%m-%d")
vol.conn.index.xts <- xts(vol.conn.index.df[,-2],order.by=vol.conn.index.df[,2])

############## SPILLOVER ###################
fevd.list.out <- rolling_var[["spill"]]

fevd.df <- data.frame(unlist(fevd.list.out))
colnames(fevd.df) = c("Index")
fevd.df$Date <- as.POSIXct(rownames(fevd.df),format="%Y-%m-%d")
fevd.xts <- xts(fevd.df[,-2],order.by=fevd.df[,2])

### compare the two index measures
indice = merge.all(vol.conn.index.xts,fevd.xts)
colnames(indice) = c("Connectedness","Spillover")

chart.TimeSeries(indice,lwd=2,auto.grid=F,ylab="Index",xlab="Time",
                      main="Comparing Spillover Index levels",lty=1,
                      legend.loc="topright")
```

Figure (\@ref(fig:indexes)) shows the percentage of the 10-day ahead forecast error which can be explained by spillovers/connectedness. Mathematically, for each matrix calculated at each point in time the sum of all the off-diagonal entries are divided by the sum of the entire matrix. Surpisingly, the indexes are very correlated, hence the use of orthogonal shocks yields a similar amount of forecast error as correlated shocks in this instance.

```{r fincrisis, fig.cap="Financial Interconnectedness: 2008-10-01"}
knitr::include_graphics("C:\\Users\\Rhamey\\Desktop\\financeR\\ConnectedNess\\financial_crisis_20008_10_01.png")
```

```{r coronavirus, fig.cap="Financial Interconnectedness: 2020-03-09"}
knitr::include_graphics("C:\\Users\\Rhamey\\Desktop\\financeR\\ConnectedNess\\coronavirus_03_09_2020.png")
```

Using the rolling window it is possible to see the network at various points in time. Figure (\@ref(fig:fincrisis)) shows the network formation at the beginning of the 2008 financial crisis. AIG's central role in underwriting mortgage derivatives to large banks stands out. Figure (\@ref(fig:coronavirus)) shows a similar dynamic with subtle changes. AIG's strong connection to Credit Suisse (CS) can be explianed by recent issues of mortgage backed securities from AIG using CS's underwriting platform. Moreover, AIG's exposure to insurance claims in the wake of the coronoavirus likely adds risk to other future liabilities.

## Section 4.2: Total Directional Connectedness and Spillover

Using the time-series of connectedness and spillover tables it is also possible to see how much volatility each stock gave or received, and which ones were net givers/receivers of volatility over time as in Figures (\@ref(fig:volGive)) - (\@ref(fig:volNet)). As with the indexes, the red lines are the spillover measures and the black lines are connectedness.

These results show the consequences of the assumptions behind the SVAR and VAR models. During the run up to the 2008 financial crisis and in its wake, AIG underwrote several option contracts for various asset-backed securities with the large banks and its volatility "giving" (the black line) spikes during that time period. However, the SVAR (red line) does not pick up on that connection, unlike the "non-orthogonal" VAR, and instead estimates a lower net volatility spillover of AIG stock price to the other volatilities. A likely reason is that SVARs are sensitive to the ordering of variables used to satisfy the normality assumption, and, perhaps, a different ordering would deliver a different result.

```{r}

net.df.list <- rolling_var[["net_df"]]
spill.df.list <- rolling_var[["spillover_table"]]

net.cols <- names(net.df.list)

to.selected.net.df.list <- llply(net.df.list,function(df){df[["To"]]})
from.selected.net.df.list <- llply(net.df.list,function(df){df[["From"]]})
net.selected.net.df.list <- llply(net.df.list,function(df){df[["Net"]]})

to.selected.net.df <- as.data.frame(do.call(rbind,lapply(to.selected.net.df.list, function(x) t(data.frame(x)))))
from.selected.net.df <- as.data.frame(do.call(rbind,lapply(from.selected.net.df.list, function(x) t(data.frame(x)))))
net.selected.net.df <- as.data.frame(do.call(rbind,lapply(net.selected.net.df.list, function(x) t(data.frame(x)))))

to.selected.net.df <- to.selected.net.df %>% mutate(Date = as.POSIXct(net.cols,format="%Y-%m-%d"))
from.selected.net.df <- from.selected.net.df %>% mutate(Date = as.POSIXct(net.cols,format="%Y-%m-%d"))
net.selected.net.df <- net.selected.net.df %>% mutate(Date = as.POSIXct(net.cols,format="%Y-%m-%d"))

spill.cols <- names(spill.df.list)

to.selected.spill.df.list <- llply(spill.df.list,function(df){df[["To"]]})
from.selected.spill.df.list <- llply(spill.df.list,function(df){df[["From"]]})
net.selected.spill.df.list <- llply(spill.df.list,function(df){df[["Net"]]})

to.selected.spill.df <- as.data.frame(do.call(rbind,lapply(to.selected.spill.df.list, function(x) t(data.frame(x)))))
from.selected.spill.df <- as.data.frame(do.call(rbind,lapply(from.selected.spill.df.list, function(x) t(data.frame(x)))))
net.selected.spill.df <- as.data.frame(do.call(rbind,lapply(net.selected.spill.df.list, function(x) t(data.frame(x)))))

to.selected.spill.df <- to.selected.spill.df %>% mutate(Date = as.POSIXct(spill.cols,format="%Y-%m-%d"))
from.selected.spill.df <- from.selected.spill.df %>% mutate(Date = as.POSIXct(spill.cols,format="%Y-%m-%d"))
net.selected.spill.df <- net.selected.spill.df %>% mutate(Date = as.POSIXct(spill.cols,format="%Y-%m-%d"))

to.net.melt <- melt(to.selected.net.df,id="Date",value.name = "Connectedness")
to.spill.melt <- melt(to.selected.spill.df,id="Date",value.name = "Spillover")
to.tbl <- merge(to.net.melt,to.spill.melt,by=c("Date","variable"))

from.net.melt <- melt(from.selected.net.df,id="Date",value.name = "Connectedness")
from.spill.melt <- melt(from.selected.spill.df,id="Date",value.name = "Spillover")
from.tbl <- merge(from.net.melt,from.spill.melt,by=c("Date","variable"))

net.net.melt <- melt(net.selected.net.df,id="Date",value.name = "Connectedness")
net.spill.melt <- melt(net.selected.spill.df,id="Date",value.name = "Spillover")
net.tbl <- merge(net.net.melt,net.spill.melt,by=c("Date","variable"))
    
```

### Giving

```{r volGive, fig.width=14,fig.height=12,fig.cap="Volatility Giving"}
ggplot(to.tbl) + 
  geom_line(aes(x=Date,y=Connectedness),colour="black") +
  geom_line(aes(x=Date,y=Spillover),colour="red") +
  facet_wrap(~variable,scales="free_y") +
  theme(legend.position = "bottom")
```

### Receiving

```{r volRec, fig.width=14,fig.height=12,fig.cap="Volatility Receiving"}
ggplot(from.tbl) + 
  geom_line(aes(x=Date,y=Connectedness),colour="black") +
  geom_line(aes(x=Date,y=Spillover),colour="red") +
  facet_wrap(~variable,scales="free_y") +
  theme(legend.position = "bottom")
```

### Net

```{r volNet, fig.width=14,fig.height=12,fig.cap="Net Volatility Connectedness"}
ggplot(net.tbl) + 
  geom_line(aes(x=Date,y=Connectedness),colour="black") +
  geom_line(aes(x=Date,y=Spillover),colour="red") +
  facet_wrap(~variable,scales="free_y") +
  theme(legend.position = "bottom")
```

## Section 4.3: Rolling Window Net Pairwise Spillovers & Connectedness

Using the connectedness measure, it is also possible to compare two stocks and see which one was the net giver or receiver. For instance Figure (\@ref(fig:pairwise)) shows that Morgan Stanley "gave" volatility to Goldman Sachs for most of the time considered.

```{r pairwise, fig.width=14,fig.height=8,fig.cap="Pairwise Connectedness between Morgan Stanley(MS) and Goldman Sachs(GS)"}
sender <- "MS"
receiver <- "GS"

net_stuff <- rolling_var[["net_df"]]
tables <- llply(net_stuff,function(stuff) stuff[["table"]])

pairwise_data <- ldply(tables,function(table){
  sender_index <- which(colnames(table) == sender)
  receiver_index <- which(colnames(table) == receiver)
  
  sender_shock <- table[receiver_index,sender_index]
  receiver_shock <- table[sender_index,receiver_index]
  
  net_shock <- sender_shock - receiver_shock
})
colnames(pairwise_data) <- c("Time","Connectedness")
pairwise_data <- pairwise_data %>% mutate(Time = as.POSIXct(Time,format="%Y-%m-%d"))

ggplot(pairwise_data,aes(x=Time,y=Connectedness,group=1)) + 
  geom_line() +
  geom_hline(yintercept=0,color="red",linetype="dashed")+
  ggtitle(paste(sender,"to", receiver)) + 
  xlab("Time") + 
  ylab("Connectedness")
```

## Section 5: Monetary Transmission

Research from [Niepmann & Schmidt-Eisenlohr](https://voxeu.org/article/when-dollar-appreciates-us-corporate-credit-tightens) shows that when the US dollar appreciates business loan terms tighten. The main reason for this is that banks often package and sell their business loans in capital markets to Collateralized Loan Obligations (CLO), insurance companies, mutual funds, hedge funds, and sometimes other banks, which typically have exposure to foreign exchange markets. Hence, when the dollar appreciates these entities will buy fewer business loans, and this lack of demand causes banks to tighten loan terms so as to offer better returns in the capital markets. According to Niepmann & Schmidt-Eisenlohr, this phenomenon is strongest among emerging market currencies. This development is significant for several reasons, but perhaps the most salient point is that according to economic theory currency appreciation is associated with lower asset reterns (i.e. lower interest rates), as Figure (\@ref(fig:krugman)) from [Krugman, Obstfeld, and Melitz](https://www.amazon.com/International-Economics-Theory-Policy-Pearson/dp/0133423646) demonstrates:

```{r krugman, fig.cap="Exchange Rates, Interest Rates, and Money Supply"}
knitr::include_graphics("C:\\Users\\Rhamey\\Desktop\\financeR\\ConnectedNess\\krugman_obstfeld.png")
```
Using the methodology above and few proxies for relevant financial assets (CLOs, exchange rates, money markets, and oil) we can get an idea of how connected loan demand and foreign exchange are. According to theory when interest rates fall the dollar will appreciate and lower returns, hence a proxy investment for dollar exchange rate, relative to EMs and developed markets, and money market returns, to measure money demand, are necessary. Also, a market price index for CLOs will help estimate returns for buyers and sellers of this asset relative to foreign exchange rates. The price of oil has been found to be an [important factor](https://www.bankofcanada.ca/wp-content/uploads/2010/05/wp10-5.pdf) driving exchange rates, thus the price of WTI could also be helpful. Finally, Citibank, JP Morgan, and Wells Fargo are among the largest sellers (buyer in Citibank's case) as discussed in a [note from S&P](https://www.spglobal.com/marketintelligence/en/news-insights/latest-news-headlines/leveraged-loan-news/those-700b-in-us-clos-who-holds-them-what-risk-they-pose). 

Luckily there is an ETF for everything as Table 4 shows.

```{r echo=FALSE,message=FALSE}
tickers <- c("BSCK","VCSH","USO","LEMB","FXCH","UUP","FFRHX","OXLC","C","WFC","JPM")
desc <- c("Money Market Proxy (ETF)","Money Market Proxy (ETF)","Light, sweet crude (ETF)","Emerging Market Currencies (excl. China) (ETF)","Chinese/USD Yuan (ETF)","USD Index relative to developed economies (ETF)","Fidelity Floating Rate High Income Fund (Closed-End Fund)","Oxford Lane Corporation (Closed End Fund)","Citibank","Wells Fargo","JP Morgan Chase & Co.")

tickerTable <- data.frame(Ticker=tickers,Description=desc)
kable(tickerTable,caption = "Stocks and Funds") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

Using the same inputs for the rolling window size, window increment, AR lag, MA order, spillovers and connectedness are calculated over the time period spanning April 2, 2012 to March 13, 2020. Charts summarisng the (log) price, cumulative returns, annualized daily volatility, and daily returns are shown in Figures \@ref(fig:logp) - \@ref(fig:cloret)

```{r echo=FALSE,message=FALSE,warning=FALSE}
tickers <- tickers[order(tickers)]

start_date = "2012-04-02"
end_date = "2020-03-13"

dat2 <- new.env()
getSymbols(tickers,
               src="yahoo",
               from=start_date,
               to = end_date,
               env=dat2)
    
logprices <- lapply(dat2,log_prices)
logprices <- Reduce(merge.all,logprices)


cum_returns <- lapply(dat2,cumulative_return)
cum_returns <- Reduce(merge.all,cum_returns)

rets <- lapply(dat2,simple.ret)
vol <- lapply(dat2,vol.fn)
## combine each ticker's xts object into one xts object
vol.data <- Reduce(merge.all,vol)
vol.data <- na.omit(vol.data)
ret.data <- Reduce(merge.all,rets)
ret.data <- na.omit(ret.data)

ret1.names <- colnames(vol.data)
## replace names with actual ticker symbols
newNames <- sapply(as.character(ret1.names),function(x) substr(x,1,nchar(x)-4))
colnames(ret.data) <- newNames
colnames(vol.data) <- newNames
colnames(cum_returns) <- newNames
colnames(logprices) <- newNames
    
all_data <- list(log_price = logprices,
         returns = ret.data,
         cum_returns = cum_returns,
         return_vol = vol.data)

monetary_transmission <- vector_autoreg(vol.data)

```

## Price History (Log Prices)

In Figure (\@ref(fig:logp)), the black line in the graph that falls precipitously is USO (oil), the other black line is UUP (USD exhange rate with other developed economies), and the green line at the bottom is Oxford Lane Capital, a purchaser of CLO securities. Of course the dollar and oil are negatively correlated, but it should be surprising that the prices of oil and OXLC move together. Moreover, it appears that as the dollar appreciates against the yen (FXHC, top blue line) and other emerging markets (LEMB, pink line) OXLC also falls in price.

```{r logp, fig.width=14,fig.height=12,fig.cap="Log Prices"}

chart.TimeSeries(all_data[["log_price"]],lwd=2,auto.grid=F,ylab="Log Prices",xlab="Time",
                     main="Log Prices",lty=1,
                     legend.loc="topright")
```

## Cumulative Returns

```{r cumret, fig.width=14,fig.height=12,fig.cap="Cumulative Returns"}
chart.TimeSeries(all_data[["cum_returns"]],lwd=2,auto.grid=F,ylab="Cumulative Returns",xlab="Time",
                     main="Returns (%)",lty=1,
                     legend.loc="topright")
```
                     
## Daily Volatility

```{r retvol, fig.width=14,fig.height=12,fig.cap="Annualized Volatility"}
chart.TimeSeries(all_data[["return_vol"]],lwd=2,auto.grid=F,ylab="Annualized Log Volatility",xlab="Time",
                     main="Log Volatility",lty=1,
                     legend.loc="topright")
```

## Daily Returns

```{r cloret, fig.width=14,fig.height=12,fig.cap="Daily Returns"}
chart.TimeSeries(all_data[["returns"]],lwd=2,auto.grid=F,ylab="Return (%)",xlab="Time",
                     main="Daily Return Percentage",lty=1,
                     legend.loc="topright")
```

### Network

```{r}
vol_var = VAR(vol.data,p=3,type="none")
amat <- diag(ncol(vol.data))
amat[lower.tri(amat)] <- NA
### extract residuals of the VAR
res_t <- residuals(vol_var)
MA_lag <- 10
theta_temp <- Phi(vol_var,nstep = MA_lag)
### extract MA coefficients
theta.list <- alply(theta_temp,3)
dir.mat <- directional.matrix(res_t,theta.list)
rownames(dir.mat) <- colnames(vol.data)
colnames(dir.mat) <- rownames(dir.mat)

D <- normalize(dir.mat)*100
df.list <- make.table(D,rownames(D),start_date)
df.list <- round(df.list[["table"]],2)

kable(df.list,caption = "Monetary Transmission Connectedness") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


```{r monetarynet, fig.width=14,fig.height=12,fig.cap="Monetary Transmission Network"}
net.mat = t(D) - D
net.melt1 <- melt(net.mat)
net.melt <- net.melt1 %>% filter(value != 0 )
colnames(net.melt)[3] <- "weight"

### calculate each percentile of the net pairwise connectedness values
### and choose only the top 10%
net.quant <- quantile(net.melt$weight,prob=seq(0,1,by=0.01))
new.net1 <- net.melt[net.melt$weight >= net.quant[90],]
new.net <- new.net1[new.net1[,1] != new.net1[,2],]

### create igraph graph
net.network <- graph.data.frame(new.net,direct=T)
#net.network <- set_edge_attr(net.network, "weight", value= new.net$weight)
E(net.network)$weight <- as.numeric(new.net$weight)
### set graph nodes
#V(net.network)
### set edge colors
E(net.network)$color <- ifelse(E(net.network)$weight >= net.quant[99],"black",
                             ifelse(E(net.network)$weight < net.quant[99] & E(net.network)$weight >= net.quant[95],"red",
                                    ifelse(E(net.network)$weight < net.quant[95] & E(net.network)$weight >= net.quant[90],"orange","blue")))

### set node size
V(net.network)$size <- degree(net.network)/.5

plot(net.network,layout=layout.circle(net.network),
     main="Firm Connectedness \n (intra day return volatility)",
     xlab = "black: 1st percentile \n red: 5th percentile \n orange: 10th percentile \n node size: number of edeges connected to the node")
```

Figure (\@ref(fig:monetarynet)) shows the connections in the top tenth percentile. Table 5 shows that the price of oil is the main driver of this system, followed by Citibank, JPM, and OXLC. Overall, the network illustrates the expected behavior of the systeme based on what we know about its players and some interesting nuances. While Citibank and OXLC purchase CLOs from the likes of JPM it is the volatility of C, OXLC, and USO (oil) that appear to drive the volatilities of exchange rates and money markets as Niepmann & Schmidt-Eisenlohr described. Along with oil prices we could add CLO demand to the list of drivers of exchange rates and money market returns.


```{r oilselloff, fig.cap="Oil Sell Off: 2014-12-10"}
knitr::include_graphics("C:\\Users\\Rhamey\\Desktop\\financeR\\ConnectedNess\\oil_clo_fx_12_2014.png")
```

```{r oilcorona, fig.cap="Coronavirus Selloff: 2020-03-05"}
knitr::include_graphics("C:\\Users\\Rhamey\\Desktop\\financeR\\ConnectedNess\\oil_clo_fx_03_05_2020.png")
```

The relationships shown in Figure (\@ref(fig:monetarynet)) are also evident in Figure (\@ref(fig:oilselloff)) which shows the effects oil selloff from 2014 year end. OXLC is driving volatilities in money markets and USD/RMB exchange rates and USO volatility is spilling over into everything else. Figure (\@ref(fig:oilselloff)) shows the same network just as the market selloff from coronavirus was starting. Somewhat surprisingly Citibank is primary driver of volatility spillovers. USO and OXLC only effect one other node each - money markets and USD/RMB exhange rate. Citibank is a large bank and likely has large exposures across all asset classes. However, in the context of this network it is the largest purchaser of CLOs which have a higher likelihood of default in the wake of a pandemic, and neither JPM nor WFC, large banks themselves, large sources of volatility spillovers. Perhaps, this analysis is saying something about the percieved risk in Citibank's balance sheet in the current crisis relative to other banks, a claim which is also finds support in Figure (\@ref(fig:coronavirus)).


### Volatility Index

Financial stocks are highly correlated, but it should come as a surprise that these variables are as connected as the index in Figure (\@ref(fig:monetaryIndex)) shows. However, the Connectedness Index is also very volatile here, falling from just over 80% to under 70% between 2015 - 2017, before jumping up above 80 again.

```{r monetaryIndex, fig.width=14,fig.height=8,fig.cap="Monetary Transmission Connectedness Index"}
############## CONNECTEDNESS ################
vol.conn.index.out <- monetary_transmission[["conn"]]
### unlist output data
vol.conn.index.df <- data.frame(unlist(vol.conn.index.out))
colnames(vol.conn.index.df) = c("Index")
### .xts object
vol.conn.index.df$Date <- as.POSIXct(rownames(vol.conn.index.df),format="%Y-%m-%d")
vol.conn.index.xts <- xts(vol.conn.index.df[,-2],order.by=vol.conn.index.df[,2])

############## SPILLOVER ###################
fevd.list.out <- monetary_transmission[["spill"]]

fevd.df <- data.frame(unlist(fevd.list.out))
colnames(fevd.df) = c("Index")
fevd.df$Date <- as.POSIXct(rownames(fevd.df),format="%Y-%m-%d")
fevd.xts <- xts(fevd.df[,-2],order.by=fevd.df[,2])

### compare the two index measures
indice = merge.all(vol.conn.index.xts,fevd.xts)
colnames(indice) = c("Connectedness","Spillover")

chart.TimeSeries(indice,lwd=2,auto.grid=F,ylab="Index",xlab="Time",
                      main="Comparing Spillover Index levels",lty=1,
                      legend.loc="topright")
```

### Net Volatility Connectedness

Figure (\@ref(fig:monetarynetvol)) of the net volatility spillovers shows why even impsoing independence on correlated variables is not always feasible. The prices of VCSH, FFRHX, and BSCK show hardly any signs of life, but the FEVD from the orthogonal SVAR of these ETFs is high and volatile. In almost every other instance, the SVAR assigns an asset as giver (receiver) of volatility when the non-orthogonal VAR assigns the same asset as a receiver (giver).  After considering these graphs, it is all the more surprising that both the Connectedness and Spillover indexes track each other as well as they do. Overall, using a non-orthogonal VAR does not necessarily reduce the total amount of error in a forecast but it does capture the key drivers of forecast errors.

```{r monetarynetvol, fig.width=14,fig.height=12,fig.cap="Monetary Transmission Connectedness"}

net.df.list <- monetary_transmission[["net_df"]]
spill.df.list <- monetary_transmission[["spillover_table"]]

net.cols <- names(net.df.list)
net.selected.net.df.list <- llply(net.df.list,function(df){df[["Net"]]})
net.selected.net.df <- as.data.frame(do.call(rbind,lapply(net.selected.net.df.list, function(x) t(data.frame(x)))))
net.selected.net.df <- net.selected.net.df %>% mutate(Date = as.POSIXct(net.cols,format="%Y-%m-%d"))

spill.cols <- names(spill.df.list)
net.selected.spill.df.list <- llply(spill.df.list,function(df){df[["Net"]]})
net.selected.spill.df <- as.data.frame(do.call(rbind,lapply(net.selected.spill.df.list, function(x) t(data.frame(x)))))
net.selected.spill.df <- net.selected.spill.df %>% mutate(Date = as.POSIXct(spill.cols,format="%Y-%m-%d"))

net.net.melt <- melt(net.selected.net.df,id="Date",value.name = "Connectedness")
net.spill.melt <- melt(net.selected.spill.df,id="Date",value.name = "Spillover")
net.tbl <- merge(net.net.melt,net.spill.melt,by=c("Date","variable"))
    
ggplot(net.tbl) + 
  geom_line(aes(x=Date,y=Connectedness),colour="black") +
  geom_line(aes(x=Date,y=Spillover),colour="red") +
  facet_wrap(~variable,scales="free_y") +
  theme(legend.position = "bottom")
```

